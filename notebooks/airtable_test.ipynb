{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e8c65ee3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import os\n",
    "\n",
    "class AirtableClient:\n",
    "    def __init__(self, api_key, base_id, table_id):\n",
    "        self.api_key = api_key\n",
    "        self.base_id = base_id\n",
    "        self.table_id = table_id\n",
    "        self.base_url = f\"https://api.airtable.com/v0/{self.base_id}/{self.table_id}\"\n",
    "        self.headers = {\n",
    "            \"Authorization\": f\"Bearer {self.api_key}\",\n",
    "            \"Content-Type\": \"application/json\"\n",
    "        }\n",
    "\n",
    "    def get_records(self, params=None):\n",
    "        \"\"\"Fetch records from the Airtable table.\"\"\"\n",
    "        response = requests.get(self.base_url, headers=self.headers, params=params)\n",
    "        response.raise_for_status()\n",
    "        return response.json()\n",
    "\n",
    "    def update_record(self, record_id, fields):\n",
    "        \"\"\"\n",
    "        Update a record in the Airtable table.\n",
    "\n",
    "        :param record_id: The ID of the record to update.\n",
    "        :param fields: A dictionary of fields to update.\n",
    "        \"\"\"\n",
    "        url = f\"{self.base_url}/{record_id}\"\n",
    "        data = {\"fields\": fields}\n",
    "        response = requests.patch(url, headers=self.headers, json=data)\n",
    "        response.raise_for_status()\n",
    "        return response.json()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e5026442",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'records': [{'id': 'recDG3FwSxuvUL0jY', 'createdTime': '2025-05-16T17:13:10.000Z', 'fields': {'cv': '📄 CURRICULUM VITAE\\n\\nALEXANDER J. RUIZ\\n📍 San Francisco, CA • 📞 (415) 987-6543 • ✉️ alex.ruiz.devops@protonmail.com • 🌐 linkedin.com/in/alexjruiz-devops • 💻 github.com/ajr-devops • ☁️ alexruiz.dev\\n\\n👨\\u200d💼 Professional Summary\\nSeasoned Principal DevOps Engineer with 12+ years of experience leading the design, implementation, and optimization of cloud-native infrastructure and DevOps pipelines. Proven track record in building robust, scalable, and secure platforms for enterprise applications across fintech, SaaS, and healthcare sectors. Specialized in Kubernetes, AWS/GCP/Azure, CI/CD architecture, observability, GitOps, and site reliability engineering (SRE). Passionate about infrastructure as code, cloud security, and mentoring cross-functional teams.\\n\\n🛠️ Core Competencies\\nInfrastructure & Cloud Platforms:\\n\\nAWS (ECS, EKS, Lambda, CloudFormation, IAM, Route53, S3, RDS, CloudFront)\\n\\nGCP (GKE, BigQuery, Cloud Build, Stackdriver)\\n\\nAzure (AKS, ARM Templates, App Services)\\n\\nKubernetes (Helm, ArgoCD, Kustomize, Operators)\\n\\nCI/CD & Automation:\\n\\nJenkins, GitHub Actions, GitLab CI/CD, CircleCI\\n\\nSpinnaker, ArgoCD, Tekton, Bamboo\\n\\nBash, Python, Groovy, Go (intermediate), Terraform, Ansible, Packer\\n\\nMonitoring & Observability:\\n\\nPrometheus, Grafana, Datadog, ELK/EFK Stack, New Relic, Splunk\\n\\nOpenTelemetry, Jaeger, Loki\\n\\nSecurity & Compliance:\\n\\nHashiCorp Vault, AWS Secrets Manager\\n\\nSnyk, Trivy, Aqua Security, Twistlock\\n\\nSOC 2, HIPAA, GDPR, ISO 27001 compliance\\n\\nDevOps Practices & Tools:\\n\\nInfrastructure as Code (Terraform, Pulumi)\\n\\nConfiguration Management (Ansible, Chef, Puppet)\\n\\nGitOps, Blue-Green & Canary Deployments\\n\\nSRE practices, Chaos Engineering (Gremlin, LitmusChaos)\\n\\nService Mesh (Istio, Linkerd), API Gateways\\n\\nSoft Skills & Leadership:\\n\\nTechnical strategy & architecture\\n\\nMentorship & cross-team collaboration\\n\\nAgile/SAFe methodology\\n\\nExecutive communication & decision-making\\n\\n📜 Certifications\\n🏅 AWS Certified DevOps Engineer – Professional\\n\\n🏅 Certified Kubernetes Administrator (CKA)\\n\\n🏅 Certified Kubernetes Application Developer (CKAD)\\n\\n🏅 Google Professional Cloud DevOps Engineer\\n\\n🏅 Terraform Associate (HashiCorp)\\n\\n🏅 Certified Jenkins Engineer\\n\\n🏅 Security+ (CompTIA)\\n\\n💼 Professional Experience\\nPrincipal DevOps Engineer\\nOctane Digital Infrastructure | San Francisco, CA\\nMay 2020 – Present\\n\\nDesigned and led migration from on-premise to Kubernetes-based cloud infrastructure across AWS and GCP, improving deployment speed by 4x.\\n\\nArchitected multi-account AWS infrastructure with Terraform, IAM role assumptions, VPC peering, and service-to-service communication using App Mesh.\\n\\nEstablished enterprise CI/CD pipelines using GitHub Actions and ArgoCD with GitOps workflows, supporting 150+ microservices.\\n\\nImplemented observability stack with Prometheus, Grafana, Loki, and OpenTelemetry, reducing mean-time-to-detect (MTTD) by 65%.\\n\\nCreated golden image pipeline with Packer + HashiCorp Vault integration for hardened AMIs across dev/staging/prod.\\n\\nGuided quarterly chaos engineering drills and SRE documentation, improving incident response KPIs by 40%.\\n\\nMentored 8 DevOps and SRE engineers; established onboarding playbooks and architecture design sessions.\\n\\nKey Achievements:\\n\\nUnified secrets management strategy across AWS & GCP using Vault and KMS integrations\\n\\nCreated internal “Platform as a Service” blueprint used by all engineering teams\\n\\nReduced AWS spend by 22% via autoscaling strategies and right-sizing workloads\\n\\nLead DevOps Engineer\\nMediflow Systems (HealthTech SaaS) | Remote\\nAugust 2016 – April 2020\\n\\nLed infrastructure transformation initiative from monolithic to containerized microservices with Kubernetes on GKE.\\n\\nImplemented policy-as-code with OPA (Open Policy Agent) to enforce security & compliance rules.\\n\\nAutomated compliance scans with Jenkins + Anchore + Trivy as part of the CI/CD process.\\n\\nIntroduced Canary Deployments via Istio, reducing production rollbacks by 70%.\\n\\nBuilt real-time alerting & anomaly detection using Prometheus, Alertmanager, and Grafana alerting.\\n\\nIntegrated audit logging and access controls to align with HIPAA requirements.\\n\\nNotable Projects:\\n\\nLaunched a zero-downtime deployment framework for customer-facing services\\n\\nLed war-room response for 2020 DNS outage—restored service within 45 mins\\n\\nDelivered an internal secrets rotation dashboard used by compliance auditors\\n\\nDevOps Engineer → Senior DevOps Engineer\\nFinverse Global | New York, NY\\nJune 2013 – July 2016\\n\\nSpearheaded migration from legacy Bamboo + EC2 workflows to Jenkins + Docker + ECS.\\n\\nWrote Terraform modules to provision secure multi-tier VPCs with ALB, NAT, and Bastion hosts.\\n\\nMonitored latency and throughput metrics for trading APIs, reducing incident count by 50%.\\n\\nImplemented Blue/Green release model for critical services used by financial partners.\\n\\nCoordinated with InfoSec to perform regular threat modeling and patch management cycles.\\n\\n🎓 Education\\nBachelor of Science in Computer Engineering\\nUniversity of California, Davis\\nGraduated: May 2013\\n\\nMinor: Applied Mathematics\\n\\nRobotics Club, Cybersecurity Lab Research Assistant\\n\\nContinued Education:\\n\\nDistributed Systems with Kubernetes – Udacity Nanodegree\\n\\nAdvanced Cloud DevOps Engineer – Pluralsight Pathway\\n\\nLeadership for Engineering Managers – Stanford Online\\n\\n📦 Featured Projects & Initiatives\\n🔥 Internal DevOps Bootcamp\\n\\nDesigned 6-week curriculum and onboarding bootcamp for junior DevOps engineers covering CI/CD, Docker, K8s, and security best practices.\\n\\n🔧 IaC Module Registry\\n\\nCreated reusable Terraform modules for networking, load balancing, and RDS provisioning, adopted company-wide by 12+ engineering teams.\\n\\n🔐 Cloud Security Audit Framework\\n\\nDeveloped internal auditing scripts to scan IAM policies, public S3 buckets, open ports, and misconfigurations across 200+ cloud resources.\\n\\n📊 Developer Insights Dashboard\\n\\nBuilt Grafana dashboards showing build frequency, lead time, and failure rates to inform DORA metrics and platform performance.\\n\\n🗣️ Speaking Engagements & Publications\\n🎤 “GitOps at Scale: Lessons from the Trenches” – DevOpsDays 2023, Amsterdam\\n\\n🎤 “Infrastructure as Code: The Good, The Bad, The Terraform” – HashiConf 2022\\n\\n✍️ “CI/CD Anti-Patterns in Enterprise Pipelines” – Medium/DevOps Junction\\n\\n✍️ “Zero to Hero with ArgoCD” – CNCF Blog\\n\\n🌍 Languages & Soft Skills\\nEnglish (Native)\\n\\nPortuguese (Fluent)\\n\\nStrong written and verbal communication\\n\\nEmpathetic leadership and team mentoring\\n\\nExcellent documentation and diagramming (draw.io, Lucidchart)\\n\\n🏆 Awards & Recognition\\n🏅 “Cloud Innovator of the Year” – Octane Infrastructure, 2023\\n\\n🏅 “Top 10 Jenkins Pipelines on GitHub” – DevOps Digest 2020\\n\\n🏅 Open Source Contributor: ArgoCD, Helm Charts, Terraform Providers\\n\\n🏅 Recognized by AWS Heroes Program (Community Nominee)', 'summarized_cv': 'Jane Doe is a project manager with 8 years of experience in logistics and retail tech.'}}, {'id': 'recWbckkLWfNa3bSi', 'createdTime': '2025-05-16T17:13:10.000Z', 'fields': {'cv': '📄 CURRICULUM VITAE\\n\\nSOPHIA T. NGUYEN\\n📍 Seattle, WA • 📞 (206) 321-9988 • ✉️ s.nguyen.dev@gmail.com • 🌐 linkedin.com/in/sophia-nguyen-python • 💻 github.com/sophiatdev • 🧠 sophia.codes\\n\\n👩\\u200d💻 Professional Summary\\nInnovative and pragmatic Python Developer with over 11 years of experience building scalable backend systems, data platforms, and RESTful APIs for SaaS, fintech, and e-commerce companies. Proven expertise in Python across multiple frameworks (FastAPI, Django, Flask), designing microservices, optimizing data pipelines, and leading development teams through complex projects. Strong advocate for clean code, test automation, and agile best practices. Adept at balancing hands-on engineering with architecture, mentoring, and cross-functional collaboration.\\n\\n🛠️ Core Technical Skills\\nLanguages & Frameworks\\n\\nPython 3.x, Type Hints, AsyncIO, Cython, Bash\\n\\nDjango, FastAPI, Flask, SQLAlchemy, Pydantic\\n\\nCelery, Redis, RabbitMQ, Kafka\\n\\nPandas, NumPy, PySpark, Dask\\n\\nREST, GraphQL, gRPC\\n\\nDatabases & Storage\\n\\nPostgreSQL, MySQL, SQLite\\n\\nMongoDB, Cassandra, DynamoDB\\n\\nRedis, Memcached, Elasticsearch\\n\\nS3, MinIO, Google Cloud Storage\\n\\nDevOps & Deployment\\n\\nDocker, Kubernetes, Helm\\n\\nTerraform, Ansible\\n\\nGitLab CI/CD, GitHub Actions, Jenkins\\n\\nAWS (Lambda, SQS, RDS, ECS, CloudWatch)\\n\\nGCP (Cloud Functions, BigQuery, Pub/Sub)\\n\\nTesting & Quality\\n\\nPytest, unittest, FactoryBoy, Faker\\n\\nCoverage.py, MyPy, Flake8, Black, Pylint\\n\\nPostman, Swagger/OpenAPI, Locust\\n\\nArchitecture & Methodology\\n\\nMicroservices & Domain-Driven Design\\n\\nEvent-Driven Architecture, Message Queues\\n\\nClean Architecture, Hexagonal Architecture\\n\\nAgile (Scrum, Kanban), TDD/BDD\\n\\nSoft Skills\\n\\nTeam leadership and mentoring\\n\\nArchitectural decision making\\n\\nExcellent communication and documentation\\n\\nCode reviews and technical writing\\n\\n📜 Certifications\\n🏅 AWS Certified Developer – Associate\\n\\n🏅 PCEP – Certified Entry-Level Python Programmer\\n\\n🏅 Django for Professionals – Real Python\\n\\n🏅 Kafka Fundamentals – Confluent\\n\\n🏅 Software Architecture – O’Reilly Masterclass\\n\\n💼 Professional Experience\\nPrincipal Python Developer / Technical Lead\\nCodexCore Technologies | Remote / San Francisco, CA\\nJune 2020 – Present\\n\\nDesigned and implemented a modular data ingestion and transformation pipeline using FastAPI + Celery + Kafka, processing 5M+ records/day.\\n\\nLed microservices migration from legacy Django monolith to distributed services using FastAPI, PostgreSQL, Redis, and gRPC for inter-service comms.\\n\\nIntegrated an in-house event streaming platform (Kafka + Avro schemas), improving real-time analytics latency by 70%.\\n\\nChampioned the introduction of typed Python, Pydantic schemas, and async/await patterns across all services.\\n\\nBuilt a code generation CLI tool to scaffold new services and APIs, cutting development time by 40%.\\n\\nMentored 6 developers, ran code reviews, and drove architectural reviews with engineering leadership.\\n\\nAchievements:\\n\\nReduced CI test suite time by 60% via Docker layer caching and parallel test runners\\n\\nLed rollout of OpenAPI docs across 15 internal services\\n\\nCreated shared Python SDK for internal service-to-service communication\\n\\nSenior Python Developer\\nShopCove Inc. (E-commerce SaaS) | Seattle, WA\\nJan 2016 – May 2020\\n\\nDeveloped scalable, high-throughput APIs for catalog management, checkout workflows, and customer analytics.\\n\\nSpearheaded performance tuning of Django ORM queries and PostgreSQL indices, reducing API response time by 50%.\\n\\nIntegrated third-party APIs (Stripe, Twilio, Salesforce) using robust error handling and retry logic.\\n\\nBuilt asynchronous data enrichment pipeline using Celery and RabbitMQ to process 20+ million customer records per month.\\n\\nWrote thousands of unit/integration tests and drove TDD adoption across the team.\\n\\nKey Contributions:\\n\\nRefactored legacy monolith into modular Django apps and reusable internal packages\\n\\nIntroduced coverage-based deployment blocking and linting automation\\n\\nCo-authored internal “Python Engineering Handbook” adopted by 3+ departments\\n\\nPython Developer → Mid-Level Engineer\\nNextInsight Data Analytics | Portland, OR\\nAug 2012 – Dec 2015\\n\\nDeveloped Python ETL pipelines for ingesting and transforming data from external partners into a centralized warehouse (Redshift & PostgreSQL).\\n\\nBuilt custom dashboards using Flask + Plotly for real-time tracking of sales performance and KPIs.\\n\\nCreated reusable internal libraries for date parsing, schema validation, and API integration.\\n\\nDesigned and maintained cron-based batch processes and monitoring with Prometheus & Alertmanager.\\n\\n🎓 Education\\nBachelor of Science in Computer Science\\nUniversity of Washington – Seattle, WA\\nGraduated: May 2012\\n\\nMinor: Statistics\\n\\nCapstone: “Distributed Log Analysis in Python with Hadoop Streaming”\\n\\nContinued Education & Courses:\\n\\nAdvanced Python and AsyncIO Patterns – Pluralsight\\n\\nClean Code in Python – O’Reilly\\n\\nModern Software Architecture – Educative.io\\n\\n📦 Featured Projects & Open Source\\n📁 PyDataCleaner (OSS)\\n\\nPython library for cleaning and validating large dataframes using declarative schema rules.\\n\\n1.2k stars on GitHub, used by multiple academic research teams.\\n\\n📊 MetricsExporter (Internal)\\n\\nLightweight Python service that extracts and exposes app metrics via Prometheus endpoints.\\n\\nPlug-and-play Flask extension version adopted by 7+ teams.\\n\\n🧪 FastTestKit\\n\\nPytest plugin for mocking and testing FastAPI endpoints with autogenerated fixtures.\\n\\n🗣️ Public Speaking & Writing\\n🎤 “Scalable APIs with FastAPI and Async Python” – PyCon US 2023\\n\\n✍️ “Asyncio in the Wild: Patterns That Scale” – Real Python Guest Article\\n\\n🎤 “Modern Python Deployment Pipelines” – DevOpsDay PDX 2022\\n\\n✍️ “Data Pipelines: When Pandas Isn’t Enough” – Medium\\n\\n🌍 Languages & Tools\\nEnglish (Native)\\n\\nVietnamese (Fluent)\\n\\nGit, VSCode, JetBrains PyCharm\\n\\ndraw.io, Lucidchart, Mermaid.js\\n\\nJIRA, Confluence, Linear\\n\\n🏆 Recognition & Awards\\n🏅 “Backend Engineer of the Year” – CodexCore 2022\\n\\n🏅 “Top Contributor” – Open Source Python Utility Library 2021\\n\\n🏅 Featured in Women in Tech 2023: Backend Leaders\\n\\n🏅 GitHub Arctic Code Vault Contributor'}}, {'id': 'reciWqAeAemmGwLfd', 'createdTime': '2025-05-16T17:13:10.000Z', 'fields': {'cv': '📄 CURRICULUM VITAE\\n\\nJohnathan M. Carter\\n📍 New York, NY • 📞 (123) 456-7890 • ✉️ john.carter.analytics@email.com • 🌐 linkedin.com/in/johncarterdata • 🧠 github.com/jmcarter-ds\\n\\n🧑\\u200d💼 Professional Summary\\nResults-driven Data Analyst with 7+ years of experience specializing in Python-based analytics, data wrangling, statistical modeling, and visualization. Expert in turning raw data into actionable insights and building scalable pipelines for enterprise-level data operations. Proficient with large datasets, cloud platforms, and machine learning frameworks. Adept at translating complex analytical findings into business strategy for stakeholders across finance, healthcare, e-commerce, and technology sectors. Passionate about data storytelling, automation, and continuous learning.\\n\\n🛠️ Technical Skills\\nProgramming & Querying:\\n\\nPython (pandas, numpy, matplotlib, seaborn, scikit-learn, statsmodels, requests, PySpark)\\n\\nSQL (PostgreSQL, MySQL, T-SQL), NoSQL (MongoDB)\\n\\nR (dplyr, ggplot2, tidyr) – basic proficiency\\n\\nData Engineering & Big Data Tools:\\n\\nApache Airflow, DBT, Luigi, ETL Pipelines\\n\\nApache Spark, Hadoop\\n\\nAWS (Redshift, S3, EC2, Lambda), GCP (BigQuery), Azure Data Lake\\n\\nVisualization & Dashboarding:\\n\\nTableau, Power BI, Looker, Plotly, Dash\\n\\nStreamlit, Excel (Power Query, VBA), Google Data Studio\\n\\nMachine Learning & Statistics:\\n\\nRegression (Linear, Logistic), Time Series (ARIMA, Prophet)\\n\\nClustering (K-Means, DBSCAN), PCA, Feature Engineering\\n\\nModel evaluation (ROC-AUC, Confusion Matrix, Cross-validation)\\n\\nDevOps & Workflow:\\n\\nGit, Docker, Jupyter, Conda, CI/CD pipelines, MLflow\\n\\nAgile, Scrum, Kanban methodologies\\n\\nJira, Trello, Asana\\n\\nSoft Skills:\\n\\nExcellent communicator, fast learner, problem solver\\n\\nExperienced in stakeholder engagement and requirement gathering\\n\\nStrong documentation and presentation abilities\\n\\n🧾 Certifications\\n📜 Microsoft Certified: Data Analyst Associate (Power BI)\\n\\n📜 AWS Certified Data Analytics – Specialty\\n\\n📜 Google Professional Data Engineer\\n\\n📜 IBM Data Science Professional Certificate (Coursera)\\n\\n📜 Python for Data Science (edX, MITx)\\n\\n📜 SQL Advanced Certification (Mode Analytics)\\n\\n📜 Tableau Desktop Specialist\\n\\n💼 Professional Experience\\nSenior Data Analyst\\nDelta Analytics Solutions Inc., New York, NY\\nApril 2021 – Present\\n\\nBuilt scalable Python-based ETL pipelines ingesting 5M+ rows of transactional data daily from multiple data lakes (AWS S3 + Redshift).\\n\\nDeveloped predictive churn models using logistic regression and random forest, leading to a 17% reduction in customer attrition.\\n\\nCreated and maintained interactive dashboards in Tableau and Streamlit for real-time KPI monitoring, adopted by 10+ departments.\\n\\nLed a team of 4 junior analysts, mentoring on best practices in data cleaning, feature selection, and pipeline optimization.\\n\\nOptimized SQL queries used in daily reporting, improving performance by 45% and reducing run-time for batch jobs from 2h to 40m.\\n\\nCollaborated with data engineers to redesign data schema, reducing redundancy and improving data integrity across the pipeline.\\n\\nKey Projects:\\n\\nSales Forecasting: Integrated Prophet model into weekly sales planning, improving forecast accuracy by 23%.\\n\\nPricing Optimization Tool: Built an interactive dashboard using Dash to simulate pricing strategies, adopted by the finance department.\\n\\nData Analyst\\nHealthPoint Technologies, Remote / Chicago, IL\\nJuly 2018 – March 2021\\n\\nConducted cohort analysis and survival modeling for patient retention in digital health products.\\n\\nAutomated reports using Python (pandas, matplotlib) and generated patient adherence visualizations for clinical teams.\\n\\nWorked closely with clinicians to model outcomes based on treatment protocols, influencing changes in care paths.\\n\\nStandardized data dictionary and data governance practices in collaboration with the data stewardship team.\\n\\nTools Used:\\nPython, SQL Server, Jupyter, Plotly, Airflow, Power BI\\n\\nAchievements:\\n\\nReduced report delivery time by 60% by automating ETL and reporting processes.\\n\\nPresented data stories to leadership that helped secure $2.3M in additional funding.\\n\\nJunior Data Analyst\\neMart Digital Retail, Boston, MA\\nJanuary 2016 – June 2018\\n\\nCleaned and preprocessed large e-commerce datasets, resolving missing values, duplicates, and schema issues.\\n\\nCreated A/B test analysis pipelines in Python to evaluate performance of website redesigns and promotions.\\n\\nCollaborated with UX researchers to analyze heatmap and user session data.\\n\\nBuilt Excel VBA macros to automate weekly sales reports and invoice checks.\\n\\n🎓 Education\\nMaster of Science in Data Analytics\\nUniversity of Illinois Urbana-Champaign\\nGraduated: May 2018\\n\\nThesis: “Multivariate Time Series Analysis on Retail Demand Forecasting using ARIMA and LSTM”\\n\\nGPA: 3.92 / 4.0\\n\\nBachelor of Science in Statistics & Computer Science\\nUniversity of Massachusetts Amherst\\nGraduated: May 2016\\n\\nMinor: Economics\\n\\nDean’s List: 6 semesters\\n\\n📊 Selected Projects & Portfolio\\n🔍 Resume Matcher with NLP\\n\\nBuilt a resume/job matcher using spaCy, cosine similarity, and TF-IDF to recommend the best-fit applicants.\\n\\nDeployed with Flask + Streamlit + Heroku.\\n\\n🕵️ Data Quality Monitor\\n\\nCreated an internal tool to detect anomalies in pipeline inputs using Z-score and IQR methods; reduced incident rate by 30%.\\n\\n📦 Customer Segmentation Engine\\n\\nApplied KMeans clustering and PCA to segment 500k+ customer records, boosting campaign targeting accuracy by 22%.\\n\\n📈 Stock Pattern Classifier\\n\\nImplemented an LSTM-based classifier for technical chart patterns on historical stock data. Achieved 74% F1-score.\\n\\n🌐 Publications & Blogs\\n“Time Series Forecasting at Scale: Comparing Facebook Prophet and ARIMA” – Towards Data Science\\n\\n“Why Analysts Should Learn Airflow” – Medium / Analytics Vidhya\\n\\n“Building a Data Science Portfolio That Gets You Hired” – KDnuggets\\n\\n🗣️ Languages\\nEnglish (Native)\\n\\nSpanish (Fluent)\\n\\nGerman (Conversational)\\n\\n🏆 Awards & Recognition\\n🏅 Delta Innovation Award 2022 – For leading enterprise forecasting dashboard\\n\\n🏅 eMart Rising Star Analyst 2017\\n\\n🏅 Featured in Tableau’s “Viz of the Day” twice for innovative dashboarding'}}]}\n"
     ]
    }
   ],
   "source": [
    "# Initialize\n",
    "client = AirtableClient(api_key=os.getenv(\"AIRTABLE_API_KEY\"), base_id=\"appPa8VJ4IHfm1V5O\", table_id=\"tblF1QERP6FFNMnM1\")\n",
    "\n",
    "# Get records\n",
    "records = client.get_records()\n",
    "print(records)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "65a2153a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'id': 'recDG3FwSxuvUL0jY', 'createdTime': '2025-05-16T17:13:10.000Z', 'fields': {'cv': 'Updated cv', 'id': '2'}}\n"
     ]
    }
   ],
   "source": [
    "# Update a record\n",
    "updated = client.update_record(\"recDG3FwSxuvUL0jY\", {\"cv\": \"Updated cv\"})\n",
    "print(updated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4c58cf9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CVManager:\n",
    "    def __init__(self, airtable_client):\n",
    "        self.client = airtable_client\n",
    "\n",
    "    def retrieve_all_cvs(self):\n",
    "        \"\"\"\n",
    "        Retrieve all CV records and return a dict:\n",
    "        { record_id: (cv, summarized_cv) }\n",
    "        \"\"\"\n",
    "        records = self.client.get_records()\n",
    "        result = {}\n",
    "        for record in records.get(\"records\", []):\n",
    "            record_id = record[\"id\"]\n",
    "            fields = record.get(\"fields\", {})\n",
    "            cv = fields.get(\"cv\", \"\")\n",
    "            summarized_cv = fields.get(\"summarized_cv\", \"\")\n",
    "            result[record_id] = (cv, summarized_cv)\n",
    "        return result\n",
    "\n",
    "    def find_unsummarized_cv(self, target_cv):\n",
    "        \"\"\"\n",
    "        Find a record where the cv matches and summarized_cv is empty.\n",
    "        Returns (record_id, cv) or None if not found.\n",
    "        \"\"\"\n",
    "        cvs = self.retrieve_all_cvs()\n",
    "        for record_id, (cv, summarized) in cvs.items():\n",
    "            if cv == target_cv and not summarized:\n",
    "                return record_id, cv\n",
    "        return None\n",
    "\n",
    "    def update_summarized_cv(self, record_id, new_summary):\n",
    "        \"\"\"\n",
    "        Update a specific record with a new summarized_cv.\n",
    "        \"\"\"\n",
    "        return self.client.update_record(record_id, {\"summarized_cv\": new_summary})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "66c9ff1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated: {'id': 'recDG3FwSxuvUL0jY', 'createdTime': '2025-05-16T17:13:10.000Z', 'fields': {'cv': 'Jane Doe Resume Content Here', 'summarized_cv': 'Jane Doe is a project manager with 8 years of experience in logistics and retail tech.'}}\n"
     ]
    }
   ],
   "source": [
    "cv_manager = CVManager(client)\n",
    "\n",
    "# Find and update a CV\n",
    "result = cv_manager.find_unsummarized_cv(\"Jane Doe Resume Content Here\")\n",
    "if result:\n",
    "    record_id, cv_text = result\n",
    "    summary = \"Jane Doe is a project manager with 8 years of experience in logistics and retail tech.\"\n",
    "    updated = cv_manager.update_summarized_cv(record_id, summary)\n",
    "    print(\"Updated:\", updated)\n",
    "else:\n",
    "    print(\"No matching unsummarized CV found.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ec61753d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VacancyManager:\n",
    "    def __init__(self, airtable_client):\n",
    "        self.client = airtable_client\n",
    "\n",
    "    def retrieve_all_vacancies(self):\n",
    "        \"\"\"\n",
    "        Retrieve all vacancy records and return a dict:\n",
    "        { record_id: (vacancy, summarized_vacancy) }\n",
    "        \"\"\"\n",
    "        records = self.client.get_records()\n",
    "        result = {}\n",
    "        for record in records.get(\"records\", []):\n",
    "            record_id = record[\"id\"]\n",
    "            fields = record.get(\"fields\", {})\n",
    "            vacancy = fields.get(\"vacancy\", \"\")\n",
    "            summarized_vacancy = fields.get(\"summarized_vacancy\", \"\")\n",
    "            result[record_id] = (vacancy, summarized_vacancy)\n",
    "        return result\n",
    "\n",
    "    def find_unsummarized_vacancy(self, target_vacancy):\n",
    "        \"\"\"\n",
    "        Find a record where the vacancy matches and summarized_vacancy is empty.\n",
    "        Returns (record_id, vacancy) or None if not found.\n",
    "        \"\"\"\n",
    "        vacancies = self.retrieve_all_vacancies()\n",
    "        for record_id, (vacancy, summarized) in vacancies.items():\n",
    "            if vacancy == target_vacancy and not summarized:\n",
    "                return record_id, vacancy\n",
    "        return None\n",
    "\n",
    "    def update_summarized_vacancy(self, record_id, new_summary):\n",
    "        \"\"\"\n",
    "        Update a specific record with a new summarized_vacancy.\n",
    "        \"\"\"\n",
    "        return self.client.update_record(record_id, {\"summarized_vacancy\": new_summary})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "277987fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated: {'id': 'recSQzHNjR0SVCja8', 'createdTime': '2025-05-16T17:13:10.000Z', 'fields': {'vacancy': 'We are hiring a backend Python developer with 3+ years of experience...', 'summarized_vacancy': 'Looking for a backend Python developer with 3+ years experience.'}}\n"
     ]
    }
   ],
   "source": [
    "vacancy_client = AirtableClient(api_key=os.getenv(\"AIRTABLE_API_KEY\"), base_id=\"appPa8VJ4IHfm1V5O\", table_id=\"tblfVZLqyJjb2SVHW\")\n",
    "vacancy_manager = VacancyManager(vacancy_client)\n",
    "\n",
    "# Find and update a vacancy\n",
    "result = vacancy_manager.find_unsummarized_vacancy(\"We are hiring a backend Python developer with 3+ years of experience...\")\n",
    "if result:\n",
    "    record_id, vacancy_text = result\n",
    "    summary = \"Looking for a backend Python developer with 3+ years experience.\"\n",
    "    updated = vacancy_manager.update_summarized_vacancy(record_id, summary)\n",
    "    print(\"Updated:\", updated)\n",
    "else:\n",
    "    print(\"No matching unsummarized vacancy found.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lunapark-O9u8KUI_-py3.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
